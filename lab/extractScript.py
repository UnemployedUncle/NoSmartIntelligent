#%%
# pip install youtube_transcript_api
# https://pypi.org/project/youtube-transcript-api/
from youtube_transcript_api import YouTubeTranscriptApi

title = 'whatisRAG'
video_id = 'T-D1OfcDW1M'
srt = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
print(srt)

#%%
script = ''
for i in srt:
    script += i['text'] + '\n'

# print(script)
# Save script to txt file
script_file = open(f'Script_{title}.txt', 'w')
script_file.write(script)
script_file.close()

#%%
# Video information
transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
# transcript = transcript_list.find_transcript(['ko', 'en'])

print(
    transcript_list.video_id,
    transcript_list.language,
    transcript_list.language_code,
    # whether it has been manually created or generated by YouTube
    transcript_list.is_generated,
    # whether this transcript can be translated or not
    transcript_list.is_translatable,
    # a list of languages the transcript can be translated to
    transcript_list.translation_languages,
)
'''
(MANUALLY CREATED)
 - en-US ("English (United States)")[TRANSLATABLE]

(GENERATED)
 - en ("English (auto-generated)")[TRANSLATABLE]

(TRANSLATION LANGUAGES)
 - af ("Afrikaans")
 - ak ("Akan")
 - sq ("Albanian")
 - am ("Amharic")
 - ar ("Arabic")
 - hy ("Armenian")
 - as ("Assamese")
 '''

#%%
"""
[{'text': 'Large language models. They are everywhere.', 'start': 0.06, 'duration': 2.572},
{'text': 'They get some things amazingly right', 'start': 2.632, 'duration': 2.635},
{'text': 'and other things very interestingly wrong.', 'start': 5.267, 'duration': 2.474},
{'text': 'My name\xa0is Marina Danilevsky.', 'start': 7.819, 'duration': 1.759},
{'text': 'I am a Senior Research Scientist here at IBM Research.', 'start': 9.578, 'duration': 2.658},
{'text': 'And I want\xa0to tell you about a framework to help large language models', 'start': 12.314, 'duration': 4.235},
{'text': 'be more accurate and more up to\xa0date:', 'start': 16.549, 'duration': 2.099},
{'text': 'Retrieval-Augmented Generation, or RAG.', 'start': 18.648, 'duration': 3.252},

{'text': 'Let\'s just talk about the "Generation" part for a\xa0minute.', 'start': 22.68, 'duration': 2.104},
{'text': 'So forget the "Retrieval-Augmented".', 'start': 24.784, 'duration': 1.925},
{'text': 'So the\xa0generation, this refers to large language models,\xa0or LLMs,', 'start': 26.8, 'duration': 4.277}, {'text': 'that generate text in response to a user query, referred to as a prompt.', 'start': 31.077, 'duration': 4.732}, {'text': 'These\xa0models can have some undesirable behavior.', 'start': 36.0, 'duration': 2.269}, {'text': 'I want to tell you an anecdote to illustrate this.', 'start': 38.269, 'duration': 3.015}, {'text': 'So my kids, they recently asked me this question:', 'start': 41.284, 'duration': 3.156}, {'text': '"In our solar system, what planet has the most\xa0moons?"', 'start': 44.44, 'duration': 4.09}, {'text': "And my response was, “Oh, that's really great that you're asking this question. I loved\xa0space when I was your age.”", 'start': 48.713, 'duration': 7.035}, {'text': 'Of course, that was like 30 years ago.', 'start': 55.748, 'duration': 2.326}, {'text': 'But I know this! I read an\xa0article', 'start': 58.074, 'duration': 2.85}, {'text': "and the article said that it was Jupiter and 88 moons. So that's the answer.", 'start': 60.924, 'duration': 5.31}, {'text': "Now, actually,\xa0there's a couple of things wrong with my answer.", 'start': 66.234, 'duration': 3.76}, {'text': "First of all, I have no source to support what\xa0I'm saying.", 'start': 70.38, 'duration': 3.646}, {'text': "So even though I confidently said “I read an article, I know the answer!”, I'm not\xa0sourcing it.", 'start': 74.026, 'duration': 4.206}, {'text': "I'm giving the answer off the top of my head.", 'start': 78.232, 'duration': 2.36}, {'text': "And also, I actually haven't kept up with\xa0this for awhile, and my answer is out of date.", 'start': 80.592, 'duration': 4.668}, {'text': 'So we have two problems here. One is no source.\xa0And the second problem is that I am out of date.\xa0\xa0', 'start': 86.1, 'duration': 6.9}, {'text': 'And these, in fact, are two behaviors that are\xa0often observed as problematic', 'start': 95.4, 'duration': 6.516}, {'text': 'when interacting with large language models. They’re LLM\xa0challenges.', 'start': 101.916, 'duration': 4.372}, {'text': "Now, what would have happened if I'd taken a beat and first gone", 'start': 106.288, 'duration': 4.174}, {'text': 'and looked\xa0up the answer on a reputable source like NASA?', 'start': 110.462, 'duration': 3.718}, {'text': 'Well, then I would have been able to say, “Ah,\xa0okay! So the answer is Saturn with 146 moons.”', 'start': 115.38, 'duration': 7.872}, {'text': 'And in fact, this keeps changing because scientists\xa0keep on discovering more and more moons.', 'start': 123.252, 'duration': 4.833}, {'text': 'So I have now grounded my answer in something more\xa0\nbelievable.', 'start': 128.085, 'duration': 2.928}, {'text': 'I have not hallucinated or made up an answer.', 'start': 131.091, 'duration': 1.991}, {'text': "Oh, by the way, I didn't leak personal\xa0information about how long ago it's been since I was obsessed with space.", 'start': 133.082, 'duration': 5.371}, {'text': 'All right, so what does\xa0this have to do with large language models?', 'start': 138.753, 'duration': 3.312}, {'text': 'Well, how would a large language model have answered\xa0this question?', 'start': 142.065, 'duration': 4.431}, {'text': "So let's say that I have a user asking this question about moons.", 'start': 146.496, 'duration': 5.124}, {'text': 'A large language\xa0model would confidently say,', 'start': 151.79, 'duration': 6.131}, {'text': 'OK, I have been trained and from what I know in my parameters\xa0during my training, the answer is Jupiter.', 'start': 157.921, 'duration': 7.079}, {'text': "The answer is wrong. But, you know, we don't know.", 'start': 166.68, 'duration': 3.551}, {'text': 'The large language model is very confident in what it answered.', 'start': 170.231, 'duration': 2.598}, {'text': 'Now, what happens when you add this\xa0retrieval augmented part here?', 'start': 172.946, 'duration': 4.742}, {'text': 'What does that mean?', 'start': 177.688, 'duration': 1.437}, {'text': 'That means that now, instead of just relying\xa0on what the LLM knows,', 'start': 179.216, 'duration': 3.676}, {'text': 'we are adding a content store.', 'start': 182.892, 'duration': 2.481}, {'text': 'This could be open like the internet.', 'start': 185.556, 'duration': 2.2}, {'text': 'This\xa0can be closed like some collection of documents, collection of policies, whatever.', 'start': 187.756, 'duration': 6.368}, {'text': 'The point,\xa0though, now is that the LLM first goes and talks', 'start': 194.124, 'duration': 3.642}, {'text': 'to the content store and says,\xa0“Hey, can you retrieve for me', 'start': 197.766, 'duration': 4.693}, {'text': "information that is relevant to what the user's\xa0query was?”", 'start': 202.8, 'duration': 2.941}, {'text': "And now, with this retrieval-augmented answer, it's not Jupiter anymore.", 'start': 205.845, 'duration': 6.103}, {'text': 'We know that\xa0it is Saturn. What does this look like?', 'start': 211.948, 'duration': 3.665}, {'text': 'Well, first user prompts the LLM\xa0with their question.', 'start': 215.613, 'duration': 10.726}, {'text': 'They say, this is what my question was.', 'start': 226.482, 'duration': 1.913}, {'text': "And originally,\xa0if we're just talking to a generative model,", 'start': 228.395, 'duration': 3.583}, {'text': "the generative model says, “Oh, okay, I know\xa0the response. Here it is. Here's my response.”\xa0\xa0", 'start': 232.26, 'duration': 4.907}, {'text': 'But now in the RAG framework,', 'start': 237.6, 'duration': 2.703}, {'text': 'the generative\xa0model actually has an instruction that says, "No, no, no."', 'start': 240.303, 'duration': 4.015}, {'text': '"First, go and retrieve\xa0relevant content."', 'start': 244.318, 'duration': 4.197}, {'text': '"Combine that with the user\'s question and only then generate the\xa0answer."', 'start': 248.66, 'duration': 4.706}, {'text': 'So the prompt now has three parts:', 'start': 253.549, 'duration': 4.11}, {'text': "the instruction to pay attention to, the retrieved\xa0content, together with the user's question.", 'start': 257.659, 'duration': 5.474}, {'text': 'Now give a response. And in fact, now you can give\xa0evidence for why your response was what it was.\xa0\xa0', 'start': 263.318, 'duration': 6.137}, {'text': 'So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\xa0\xa0', 'start': 270.0, 'duration': 5.34}, {'text': "So first of all, I'll start with the out of\xa0date part.", 'start': 275.94, 'duration': 2.664}, {'text': 'Now, instead of having to retrain your model, if new information comes up, like,', 'start': 278.604, 'duration': 4.62}, {'text': "hey,\xa0we found some more moons-- now to Jupiter again, maybe it'll be Saturn again in the future.", 'start': 283.224, 'duration': 4.899}, {'text': 'All\xa0you have to do is you augment your data store with new information, update information.', 'start': 288.227, 'duration': 4.962}, {'text': "So now the next time that a user comes and asks the question, we're ready.", 'start': 293.189, 'duration': 4.497}, {'text': 'We just go ahead and retrieve the most up to date information.', 'start': 297.686, 'duration': 2.605}, {'text': 'The second problem, source.', 'start': 300.291, 'duration': 1.968}, {'text': 'Well, the large language model is now being instructed to pay attention', 'start': 302.259, 'duration': 5.176}, {'text': 'to primary source data before giving its response.', 'start': 307.435, 'duration': 3.549}, {'text': 'And in fact, now being able to give evidence.', 'start': 310.984, 'duration': 2.727}, {'text': 'This makes it less likely to hallucinate or to leak data', 'start': 313.711, 'duration': 3.65}, {'text': 'because it is less likely to rely only on information that it learned during training.', 'start': 317.361, 'duration': 4.126}, {'text': 'It also allows us to get the model to have a behavior that can be very positive,', 'start': 321.761, 'duration': 4.819}, {'text': "which is knowing when to say, “I don't know.”", 'start': 326.58, 'duration': 2.84}, {'text': "If\xa0the user's question cannot be reliably answered based on your data store,", 'start': 329.42, 'duration': 5.942}, {'text': 'the model should say,\xa0"I don\'t know," instead of making up something that is believable and may mislead the user.', 'start': 335.362, 'duration': 6.538}, {'text': 'This\xa0can have a negative effect as well though, because if the retriever is not sufficiently\xa0good', 'start': 341.9, 'duration': 5.678}, {'text': 'to give the large language model the best, most high-quality grounding information,', 'start': 347.578, 'duration': 5.484}, {'text': "then\xa0maybe the user's query that is answerable doesn't get an answer.", 'start': 353.062, 'duration': 4.225}, {'text': 'So this is actually why lots\xa0of folks, including many of us here at IBM,', 'start': 357.287, 'duration': 4.182}, {'text': 'are working the problem on both sides.', 'start': 361.469, 'duration': 2.107}, {'text': 'We are both\xa0working to improve the retriever', 'start': 363.576, 'duration': 3.184}, {'text': 'to give the large language model the best quality data on which\xa0to ground its response,', 'start': 366.76, 'duration': 6.132}, {'text': 'and also the generative part so that the LLM can give the richest, best\xa0response finally to the user', 'start': 372.892, 'duration': 6.346}, {'text': 'when it generates the answer.', 'start': 379.238, 'duration': 1.709}, {'text': 'Thank you for learning more about RAG\xa0and like and subscribe to the channel.', 'start': 381.103, 'duration': 4.576}, {'text': 'Thank you.', 'start': 385.679, 'duration': 0.541}]
"""
